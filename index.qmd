---
title: "Homework 3"
author: "[Advait Ashtikar]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
# format: html
format: pdf
editor: 
  markdown: 
    wrap: 72
---

[Link to the Github repository](https://github.com/psu-stat380/hw-3)

------------------------------------------------------------------------

::: {.callout-important style="font-size: 0.8em;"}
## Due: Thu, Mar 2, 2023 \@ 11:59pm

Please read the instructions carefully before submitting your
assignment.

1.  This assignment requires you to only upload a `PDF` file on Canvas
2.  Don't collapse any code cells before submitting.
3.  Remember to make sure all your code output is rendered properly
    before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter
before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine
Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset
from the UCI Machine Learning Repository. The dataset consists of red
and white *vinho verde* wine samples, from the north of Portugal. The
goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{r}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
```

<br><br><br><br>

## Question 1

::: callout-tip
## 50 points

Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality data-sets from the specified URLs and store them
in data frames `df1` and `df2`.

```{r}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"


df1 <- read.csv(url1, header = TRUE, sep = ";")
df2 <- read.csv(url2, header = TRUE, sep = ";")
```

------------------------------------------------------------------------

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1.  Combine the two data frames into a single data frame `df`, adding a
    new column called `type` to indicate whether each row corresponds to
    white or red wine.
2.  Rename the columns of `df` to replace spaces with underscores
3.  Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
4.  Convert the `type` column to a factor
5.  Remove rows (if any) with missing values.

```{r}
# add a new column "type" to each data frame
df1 <- df1 %>%
  mutate("type" = "White")
df2 <- df2 %>%
  mutate("type" = "Red")

# combine the two data frames into a single data frame
df <- rbind(df1, df2) %>%
  rename_with(~ gsub("\\.", "_", .x)) %>%
  select(!c(fixed_acidity, free_sulfur_dioxide))

# Converting type to a factor
df$type <- as.factor(df$type)

#removing missing value rows
df <- na.omit(df)
```

```{r}
dim(df)
```

Your output to `R dim(df)` should be

    [1] 6497   11

------------------------------------------------------------------------

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the
the difference in means (with the equal variance assumption)

1.  Using `df` compute the mean of `quality` for red and white wine
    separately, and then store the difference in means as a variable
    called `diff_mean`.

2.  Compute the pooled sample variance and store the value as a variable
    called `sp_squared`.

3.  Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and
    store its value in a variable called `t1`.

```{r}
w_mean <- mean(df$quality[df$type == 'White'])
r_mean <- mean(df$quality[df$type == 'Red'])
diff_mean <- mean(w_mean - r_mean)

n1 <- length(df$quality[df$type == 'White'])
n2 <- length(df$quality[df$type == 'Red'])

v1 <- var(df$quality[df$type == 'White'])
v2 <- var(df$quality[df$type == 'Red'])

sp_squared <- ((n1-1)*v1 + (n2-1)*v2) / (n1+n2-2)

s1 <- sd(df$quality[df$type == 'White'])
s2 <- sd(df$quality[df$type == 'Red'])

t1 <- diff_mean / sqrt((sp_squared*(1/n1 + 1/n2)))
t1
```

------------------------------------------------------------------------

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to
perform a two-sample $t$-Test without having to compute the pooled
variance and difference in means.

Perform a two-sample t-test to compare the quality of white and red
wines using the `t.test()` function with the setting `var.equal=TRUE`.
Store the t-statistic in `t2`.

```{r}
t_test <- t.test(df$quality[df$type == "White"], df$quality[df$type == "Red"], var.equal = TRUE) 
t2 <- t_test$statistic
t2
```

------------------------------------------------------------------------

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the
`lm()` function, and extract the $t$-statistic for the `type`
coefficient from the model summary. Store this $t$-statistic in `t3`.

```{r}
fit <- lm(quality ~ type, data = df)
t3 <- summary(fit)$coefficients[2, "t value"]
t3
```

------------------------------------------------------------------------

###### 1.6 (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can
you conclude from this? Why?

```{r}
c(t1, t2, t3)
```

> Based on the values of `t1`, `t2`, and `t3`, we can conclude that all
> three values are the same. We can also conclude that there is a
> substantial difference between the qualities of red and white wines.
> This is because linear regression determines the linear correlation
> between the predictor and response variables whereas the $t$-test
> determines the linear relationship. Hence, they are the same

<br><br><br><br> <br><br><br><br> ---

## Question 2

::: callout-tip
## 25 points

Collinearity
:::

------------------------------------------------------------------------

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response
variable `quality`. Use the `broom::tidy()` function to print a summary
of the fitted model. What can we conclude from the model summary?

```{r}
#fitting the linear regresion model
model_all <- lm(quality ~ ., data = df)

#printing the summary of the fitted model
library(broom)
summary_all <- broom::tidy(model_all)
summary_all
```

> From the model summary, we can see that several of the predictor
> variables have a statistically significant effect on the quality of
> wine, as indicated by their $p$-value. On the other hand, the
> `fixed_acidity` and `free_sulfur_dioxide` predictors have been removed
> due to their $p$-value.

------------------------------------------------------------------------

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only
`citric_acid` as the predictor, and another with only
`total_sulfur_dioxide` as the predictor. In both models, use `quality`
as the response variable. How does your model summary compare to the
summary from the previous question?

```{r}
model_citric <- lm(quality ~ citric_acid, data = df)
summary_citric <- broom::tidy(model_citric)
summary_citric
```

```{r}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
summary_sulfur <- broom::tidy(model_sulfur)
summary_sulfur
```

> Comparing these models with the model from the previous question, we
> can see that the coefficients and $t$-Statistics for these predictors
> in the multiple regression model and simple regression model are
> consistent. However, in the multiple regression model, several other
> predictors also have an effect on the `quality`.

------------------------------------------------------------------------

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using
`corrplot()`

```{r}
library(corrplot)

numeric_cols <- df %>%
  select_if(is.numeric)

corr <- cor(numeric_cols)

corrplot(corr, method = "color")
```

------------------------------------------------------------------------

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the
full model using `vif()` function. What can we conclude from this?

```{r}
library(car)

model <- lm(quality ~ ., data = df)
vif(model)
```

> From the output, we can conclude that there is some degree of
> multicollinearity between some of the predictors in the model.
> Predictors with VIF values greater than 5, are highly correlated with
> other predictors, suggesting that their coefficients may be difficult
> to interpret.

<br><br><br><br> <br><br><br><br> ---

## Question 3

::: callout-tip
## 40 points

Variable selection
:::

------------------------------------------------------------------------

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the
starting model. Store the final formula in an object called
`backward_formula` using the built-in `formula()` function in R

```{r}
full_model <- lm(quality ~ ., data = df)
backward_formula <- step(full_model, direction = "backward", scope = formula(full_model))
```

------------------------------------------------------------------------

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the
starting model. Store the final formula in an object called
`forward_formula` using the built-in `formula()` function in R

```{r}
null_model <- lm(quality ~ 1, df)
forward_formula <- step(null_model, direction = "forward", scope = formula(full_model))
```

------------------------------------------------------------------------

###### 3.3 (10 points)

1.  Create a `y` vector that contains the response variable (`quality`)
    from the `df` dataframe.

2.  Create a design matrix `X` for the `full_model` object using the
    `make_model_matrix()` function provided in the Appendix.

3.  Then, use the `cv.glmnet()` function to perform LASSO and Ridge
    regression with `X` and `y`.

```{r}
#making y vector
y <- df$quality

#creating a design matrix
make_model_matrix <- function(formula){
  X <- model.matrix(full_model, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typeWhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}

#lasso regression and plot
lasso <- cv.glmnet(x = make_model_matrix(forward_formula), y, alpha = 1)
plot(lasso)
```

```{r}
#ridge regression and plot
ridge <- cv.glmnet(x = make_model_matrix(forward_formula), y, alpha = 0, nfolds = 5)
plot(ridge)
```

Create side-by-side plots of the ridge and LASSO regression results.
Interpret your main findings.

```{r}
#combining plots for both regressions
par(mfrow=c(1, 2))
plot(ridge, pch = 20, main = "Ridge Regression")
plot(lasso, pch = 20, main = "LASSO Regression")
```

> We can see from the graphs that the $x$-axis represents the $\lambda$
> values and the $y$ - axis displays the $mean-squared \space error$ for
> the predictor variables. We can see that as the value of $\lambda$
> increases the model becomes more regularized giving simpler
> coefficients and simpler models.

------------------------------------------------------------------------

###### 3.4 (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se`
value? What are the variables selected by LASSO?

```{r}
lasso_coef <- coef(lasso, s = "lambda.1se")
lasso_coef
```

> The variables selected by the LASSO regression are `volatile_acidity`,
> `residual_sugar`, `chlorides`, `total_sulfur_dioxide`, `pH`,
> `sulphates`, `alcohol` and `type`

Store the variable names with non-zero coefficients in `lasso_vars`, and
create a formula object called `lasso_formula` using the
`make_formula()` function provided in the Appendix.

```{r}
lasso_vars <- rownames(lasso_coef)[which(abs(lasso_coef) > 0)][-1]

make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

lasso_formula <- make_formula(lasso_vars)
lasso_formula
```

------------------------------------------------------------------------

###### 3.5 (5 points)

Print the coefficient values for ridge regression at the `lambda.1se`
value? What are the variables selected here?

```{r}
ridge_coef <- coef(ridge, s = "lambda.1se")
ridge_coef
```

Store the variable names with non-zero coefficients in `ridge_vars`, and
create a formula object called `ridge_formula` using the
`make_formula()` function provided in the Appendix.

```{r}
ridge_vars <- rownames(ridge_coef)[which(abs(ridge_coef) > 0)][-1]
ridge_formula <- make_formula(ridge_vars)

ridge_formula
```

------------------------------------------------------------------------

###### 3.6 (10 points)

What is the difference between stepwise selection, LASSO and ridge based
on you analyses above?

> Based on the analyses above, we can see that stepwise selection
> resulted in a model with eight predictors, while LASSO and ridge
> regression selected different sets of predictors. The LASSO model had
> a four non-zero coefficients, while the ridge model had non-zero
> coefficients for ten predictors, but they were smaller in magnitude
> than the coefficients in the full model.

<br><br><br><br> <br><br><br><br> ---

## Question 4

::: callout-tip
## 70 points

Variable selection
:::

------------------------------------------------------------------------

###### 4.1 (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the
covariates. How many different models can we create using any subset of
these $10$ covariates as possible predictors? Justify your answer.

> There are $2^{10} = 1024$ different possible models that can be
> created using any subset of the $10$ covariates as possible
> predictors. This is because for each of the models, we can either
> include or exclude them from the model.

------------------------------------------------------------------------

###### 4.2 (20 points)

Store the names of the predictor variables (all columns except
`quality`) in an object called `x_vars`.

```{r}
x_vars <- colnames(df %>% select(-quality))
```

Use:

-   the `combn()` function (built-in R function) and
-   the `make_formula()` (provided in the Appendix)

to **generate all possible linear regression formulas** using the
variables in `x_vars`. This is most optimally achieved using the `map()`
function from the `purrr` package.

```{r}
formulas <- map(
  1:length(x_vars),
  function(x){
    vars <- combn(x_vars, x, simplify = FALSE)
    map(vars, make_formula)
  }
) %>% unlist()
```

If your code is right the following command should return something
along the lines of:

```{r}
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

------------------------------------------------------------------------

###### 4.3 (10 points)

Use `map()` and `lm()` to fit a linear regression model to each formula
in `formulas`, using `df` as the data source. Use `broom::glance()` to
extract the model summary statistics, and bind them together into a
single tibble of summaries using the `bind_rows()` function from
`dplyr`.

```{r}
models <- map(formulas, ~lm(.x, data = df))
summaries <- map(models, broom::glance) %>%
  bind_rows()
summaries
```

------------------------------------------------------------------------

###### 4.4 (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to
identify the formula with the ***highest*** adjusted R-squared value.

```{r}
#Extracting the adj.r.squared values
adj_r_squared <- summaries$adj.r.squared

#Formula to identify the highest adjusted R-squared value
max_rsq_index <- which.max(adj_r_squared)
```

Store resulting formula as a variable called `rsq_formula`.

```{r}
rsq_formula <- formulas[which.max(adj_r_squared)]
rsq_formula
```

------------------------------------------------------------------------

###### 4.5 (5 points)

Extract the `AIC` values from `summaries` and use them to identify the
formula with the ***lowest*** AIC value.

```{r}
#Extracting the AIC values from summaries
aic_values <- summaries$AIC

#Finding the index of the formula with lowest AIC
min_aic_index <- which.min(aic_values)
```

Store resulting formula as a variable called `aic_formula`.

```{r}
aic_formula <- formulas[min_aic_index]
aic_formula
```

------------------------------------------------------------------------

###### 4.6 (15 points)

Combine all formulas shortlisted into a single vector called
`final_formulas`.

```{r}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)

```

-   Are `aic_formula` and `rsq_formula` the same? How do they differ
    from the formulas shortlisted in question 3?

    > The **`aic_formula`** and **`rsq_formula`** are not the same, as
    > they are based on different criteria for model selection.
    > **`rsq_formula`** was selected based on the highest adjusted
    > R-squared value, while **`aic_formula`** was selected based on the
    > lowest AIC value. The formulas shortlisted in question 3 were
    > obtained by exhaustively searching through all possible subsets of
    > the predictor variables, while the other methods (null, full,
    > backward, forward, LASSO, and Ridge regression) used different
    > algorithms to select a subset of variables based on certain
    > criteria.

-   Which of these is more reliable? Why?

    > In terms of which method is more reliable `rsq_formula`. With a
    > large dataset `rsq` is going to have a higher predictive power
    > than the AIC model.

-   If we had a dataset with $10,000$ columns, which of these methods
    would you consider for your analyses? Why?

    > If we had a dataset with 10,000 columns, exhaustive search through
    > all possible models would be computationally indefeasible. In this
    > case, LASSO option would be better because it can handle
    > high-dimensional datasets and automatically shrink the
    > coefficients of irrelevant predictors towards zero. Also, LASSO is
    > preferred over Ridge, because it has feature selection.

------------------------------------------------------------------------

###### 4.7 (10 points)

Use `map()` and `glance()` to extract the
`sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model
obtained from `final_formulas`. Bind them together into a single data
frame `summary_table`. Summarize your main findings.

```{}
summary_table <- map(
  final_formulas, ~ lm(.x, data = df) %>% 
    broom::glance() %>% 
    select(sigma, adj.r.squared, AIC, df, p.value)
) %>% bind_rows()

summary_table %>% knitr::kable()
```

> We can see that all the models have extremely significant $p$-values.
> The $p$-values were so small that they were nearing zero. We can also
> see that a couple of the methods came up with the same model.
> Backward, forward, and AIC methods all came up with the same model,
> while the ridge method was the same as the full model. We can also see
> that besides the null model, all the sigma values, adjusted r-squared
> values, and AIC values were extremely close for all the models.

::: {.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br> <br><br><br><br> ---

# Appendix

#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x`
and outputs a `formula` object with `quality` as the response variable
and the columns of `x` as the covariates.

```{r}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and
outputs a rescaled model matrix `X` in a format amenable for `glmnet()`

```{r}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```

::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
